import json
import pandas as pd
import numpy as np
import sqlite3
import requests
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, recall_score, accuracy_score, f1_score,roc_curve,auc
from sklearn.cluster import KMeans
import tensorflow as tf
from keras import Model
from keras.models import Sequential
from keras.api.layers import Input, Dense, LSTM, Concatenate, Dropout
from keras.api.preprocessing.sequence import pad_sequences
from keras.api.utils import plot_model
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
'''
В некотором царстве, в некотором государстве, налог на доходы физических лиц вычисляется следующим образом. Базовая ставка налога составляет 13%. Если в каком-то месяце ваш заработок за год составит больше тысячи тугриков, то на оставшуюся часть года (не включая этот месяц) устанавливается ставка в 20%. Например, если вы зарабатываете каждый месяц 150 тугриков, то к июлю заработаете 150×7=1050 тугриков и начиная с августа подоходный налог будет начисляться по ставке 20%. Написать функцию calculate_tax(income), принимающую на вход массив, содержащий доход за каждый месяц года, начиная с первого и возвращающую общую сумму налога, который предстоит заплатить за год. Год в некотором царстве может длиться более 12 месяцев, если по этому поводу будет принят соответствующий высочайший декрет.
Подсказка. Вам поможет функция np.cumsum(). Чтобы создать новый массив, длина которого равна длине какого-то другого, можно использовать функции np.zeros_like() или np.ones_like().
'''
import os
import pickle
import tqdm
from string import punctuation

start=time.time()

test= pd.read_json(r'')
print(test)

test2=pd.read_json('ha2_diff.json')
#copy=test2.copy()
#test2=test2.append(copy,ignore_index=True)
print(test2)

def to_pd_categorical(data):
  """change the format of data to categorical"""
  X = data.copy()
  for col in X.columns:
    X[col] = pd.Categorical(X[col])
  return X

def categorical_to_texts(data):
  """Transform categorical data rows into texts as follows:
  “Var1Name_Value1 Var2Name_Value2 …”
  """
  new_data_list = []
  columns = list(data.columns)
  for line in tqdm(data.values):
    new_line = ''
    for pair in zip(columns, line):
      new_line = new_line + f'{pair[0]}_{pair[1]} '
    new_data_list.append(new_line)
  return new_data_list


test2.drop(['TLSH','TLSHORIG','diff','tlsh_hash'],axis=1,inplace=True)



for i in test.index:
    test.loc[i,['id']]=int(i)
for i in test2.index:
    test2.loc[i,['id']]=int(i)
x=test[test['id']==402]
y=test[test['id']==403]
z=test[test['id']==404]
print(x.at[402,'user-agent'])
print(y.at[403,'user-agent'])
print(z.at[404,'user-agent'])


categorical_cols = [col for col in test2.columns if col != 'id']

X_train = test[categorical_cols]
y_train = test['id']
X_test = test2[categorical_cols]
y_test = test2['id']

X_train_texts = categorical_to_texts(X_train)
X_test_texts = categorical_to_texts(X_test)
def vect2gensim(vectorizer, dtmatrix):
    """ transform sparse matrix into gensim corpus and dictionary """
    corpus_vect_gensim = gensim.matutils.Sparse2Corpus(dtmatrix, documents_columns=False)
    dictionary = gensim.corpora.dictionary.Dictionary.from_corpus(corpus_vect_gensim,
        id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))

    return (corpus_vect_gensim, dictionary)
def get_topic_vectors(model,corpus):
  """Get new features form the topic LSA model and gensim corpus"""
  num_topics = len(model[corpus[1]])
  doc_vectors = np.zeros((len(corpus), num_topics))
  for i, doc in enumerate(tqdm(corpus)):
    topics = model[doc]
    for pair in topics:
      j = pair[0]
      doc_vectors[i,j] = pair[1]
  return doc_vectors

#-----------------load model test

loaded_model = LsiModel.load('lsi.model')
vectorizer = CountVectorizer(ngram_range=(1, 2)) #min_df=10,max_df=0.2
bow_matrix = vectorizer.fit_transform(X_train_texts)

#(gensim_corpus, gensim_dict) = vect2gensim(vectorizer, bow_matrix)

gensim_dict = corpora.Dictionary.load('dict.dict')
gensim_corpus = corpora.MmCorpus('corp.corp')
bow_matrix_test = vectorizer.fit_transform(X_test_texts)
gensim_corpus_test,_ = vect2gensim(vectorizer, bow_matrix_test)
print("gensim corpus matr")

lsa_features_train = get_topic_vectors(loaded_model, gensim_corpus)
lsa_features_test = get_topic_vectors(loaded_model, gensim_corpus_test)
min=10000000
a=lsa_features_test[64]


i=0
for i in range(len(lsa_features_test)):
    dst=0
    min=1000000
    a = lsa_features_test[i]
    j=0
    for j in range(len(lsa_features_train)):
        if j !=i:
            b = lsa_features_train[j]
            dst=distance.cosine(a,b)
            if dst<min:
                min=dst
                print(f'Раст {min} ,  номер test {i} с train {j}')



    #print(dst,i)

_, emb_sample, _, y_target = train_test_split(lsa_features_train , y_train, test_size=0.7)
_, emb_sample2, _, y_target2 = train_test_split(lsa_features_test , y_test, test_size=0.9)
print(len(emb_sample))
print(lsa_features_train[22])
print(lsa_features_test[22])
scaler = StandardScaler()
scaler2 = StandardScaler()
scaler.fit(lsa_features_test)
#scaler2.fit(lsa_features_train)
scaled_emb = scaler.transform(emb_sample)
scaled2_emb = scaler.transform(emb_sample2)
tsne = TSNE(n_components=2, random_state=33)
T = tsne.fit_transform(scaled_emb)
E= tsne.fit_transform(scaled2_emb)
plt.figure(figsize=(12,8))
plt.scatter(T[:,0], T[:,1], alpha=0.7, c = '#FF8C00')
plt.scatter(E[:,0], E[:,1], alpha=0.7, c = '#00008B', s=100)
labels = y_test
#for label, x, y in zip(labels, T[:,0], T[:,1]):
#    plt.annotate(label, (x,y), xycoords = 'data')
plt.grid()
plt.show()

time.sleep(100)


#------------------train model
vectorizer = CountVectorizer(ngram_range=(1, 2),min_df=10,max_df=0.2)
bow_matrix = vectorizer.fit_transform(X_train_texts)

(gensim_corpus, gensim_dict) = vect2gensim(vectorizer, bow_matrix)

bow_matrix_test = vectorizer.transform(X_test_texts)
gensim_corpus_test,_ = vect2gensim(vectorizer, bow_matrix_test)
gensim_dict.save('G:\Models\dict_2000.dict')
corpora.MmCorpus.serialize('G:\Models\corp_2000.corp', gensim_corpus)
print("gensim corpus matr save \n start lsi")


lsamodel = gensim.models.LsiModel(gensim_corpus, num_topics=2000, id2word = gensim_dict, power_iters=30)
lsamodel.save("G:\Models\lsi_clear_2000.model")
print("lsamodel")
end=time.time()
razn=(end-start)/60
print(razn)



print("Start train logistic reg")



model_lr = LogisticRegression().fit(lsa_features_train, y_train)

pkl_filename = "pickle_model.pkl"
with open(pkl_filename, 'wb') as file:
    pickle.dump(model_lr, file)
print("end train model")
probs = model_lr.predict_proba(lsa_features_test)
score_lr_lsi = roc_auc_score(y_test, probs[:,1])
print(score_lr_lsi)
time.sleep(100)



features = pd.get_dummies(test)
#features2=pd.get_dummies(test2)

feature_list = list(features.columns)

#feature_list2=list(features2.columns)


for i in features.index:
    features.loc[i,['id']]=int(i)
#for i in features2.index:
#    features2.loc[i,['id']]=22



print(features)
#print(features2)

#Z1=features2.loc[:,'color_depth':'WindowText_rgb(0, 0, 0)м0']
#Z2=features2['id']

#определяем исходные данные X и таргеты Y
X=features.loc[:,'color_depth':'WindowText_rgb(53, 64, 74)']
Y=features['id']

#разделяем исходные данные на тренировочную и тестовую выборку
X_train,X_test,Y_train,Y_test=train_test_split(X,Y, test_size=0.3, random_state=42)

from sklearn.ensemble import RandomForestRegressor

#инициализируем модель и обучаем ее
clf=RandomForestRegressor(n_estimators = 30, random_state = 42)
clf=clf.fit(X_train,Y_train)

#извлекаем оценку точности модели
print("Accuracy train set: {:.3f}".format(clf.score(X_train,Y_train)))
print("Accuracy test set: {:.3f}".format(clf.score(X_test,Y_test)))

y_pred=clf.predict(X_test)
print(y_pred)
print(Y_train)



importances = list(clf.feature_importances_)
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
#print(feature_importances[0:10])

#features.to_excel(r'K:\mag\наработки\АНАлиз\ML\no_hash_test_dum.xlsx')

print(test)
for i in range(6000):
    try:
        test = test.drop([i])
    except:
        print(i)
print(test)


#print(features.iloc[:,7:].head(7))

sequence_length = 100
BATCH_SIZE = 128
EPOCHS = 100
FILE_PATH = "sample_data/eshop.txt"
BASENAME = os.path.basename(FILE_PATH)
text = open(FILE_PATH, encoding="utf-8").read()
text = text.lower()
text = text.translate(str.maketrans("", "", punctuation))
n_chars = len(text)
vocab = ''.join(sorted(set(text)))
print("unique_chars:", vocab)
n_unique_chars = len(vocab)
print("Number of characters:", n_chars)
print("Number of unique characters:", n_unique_chars)
char2int = {c: i for i, c in enumerate(vocab)}
int2char = {i: c for i, c in enumerate(vocab)}
pickle.dump(char2int, open(f"{BASENAME}-char2int.pickle", "wb"))
pickle.dump(int2char, open(f"{BASENAME}-int2char.pickle", "wb"))
encoded_text = np.array([char2int[c] for c in text])
char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)
for char in char_dataset.take(8):
    print(char.numpy(), int2char[char.numpy()])
sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)
def split_sample(sample):
    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))
    for i in range(1, (len(sample)-1) // 2):
        input_ = sample[i: i+sequence_length]
        target = sample[i+sequence_length]
        other_ds = tf.data.Dataset.from_tensors((input_, target))
        ds = ds.concatenate(other_ds)
    return ds
# prepare inputs and targets
dataset = sequences.flat_map(split_sample)
def one_hot_samples(input_, target):
    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)
dataset = dataset.map(one_hot_samples)
for element in dataset.take(2):
    print("Input:", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))
    print("Target:", int2char[np.argmax(element[1].numpy())])
    print("Input shape:", element[0].shape)
    print("Target shape:", element[1].shape)
    print("="*50, "\n")
ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)
model = Sequential([
    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),
    Dropout(0.3),
    LSTM(256),
    Dense(n_unique_chars, activation="softmax"),])
model_weights_path = f"results/{BASENAME}-{sequence_length}.h5"
model.summary()
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
if not os.path.isdir("results"):
  	  os.mkdir("results")
model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)
model.save(model_weights_path)
seed = "интернетмагазин"
char2int = pickle.load(open(f"{BASENAME}-char2int.pickle", "rb"))
int2char = pickle.load(open(f"{BASENAME}-int2char.pickle", "rb"))
vocab_size = len(char2int)
model = Sequential([
    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),
    Dropout(0.3),
    LSTM(256),
    Dense(vocab_size, activation="softmax"),
])
model.load_weights(f"results/{BASENAME}-{sequence_length}.h5")
s = seed
n_chars = 400
generated = ""
for i in tqdm.tqdm(range(n_chars), "Generating text"):
    X = np.zeros((1, sequence_length, vocab_size))
    for t, char in enumerate(seed):
          X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1
    predicted = model.predict(X, verbose=0)[0]
    next_index = np.argmax(predicted)
    next_char = int2char[next_index]
    generated += next_char
    seed = seed[1:] + next_char


def calculate_tax(income:np.ndarray) -> float:
    cs = income.cumsum()
    if cs[0] > 1000: 
        i = income[1:].sum()* 0.2 + income[0].sum()* 0.13
    elif len(cs[cs>1000])>0 : 
        i = income[cs<=1000].sum()*0.13+income[cs>1000][0]*0.13+income[cs>1000][1:].sum()*0.2
    else :
      i = income.sum()*0.13  
    return i
##    le.fit(df['Sex'].astype(str))
   ## train_X['Sex'] = le.transform(train_X['Sex'].astype(str))
    ##test_X['Sex'] = le.transform(test_X['Sex'].astype(str))
    from timeit import timeit
def calculate_tax(income:np.ndarray) -> float:
    cs = income.cumsum()
    if cs[0] > 1000: 
        i = income[1:].sum()* 0.2 + income[0].sum()* 0.13
    elif len(cs[cs>1000])>0 : 
        i = income[cs<=1000].sum()*0.13+income[cs>1000][0]*0.13+income[cs>1000][1:].sum()*0.2
    else :
      i = income.sum()*0.13  
    return i
assert np.isclose(calculate_tax(np.array([150]*12)), 286.5)
assert np.isclose(calculate_tax(np.array([100]*12)), 163)
assert np.isclose(calculate_tax(np.array([50]*12)), 78)
assert np.isclose(calculate_tax(np.array([1000]*12)), 2260)

assert np.isclose(calculate_tax(np.array(range(12))*100), 1215)
assert np.isclose(calculate_tax(np.array(range(11,-1,-1))*100), 1243)
def dummy(x):
    z = 0
    for y in x:
        z += y
        z += y*0.12
        if z:
            z += y
    return z

assert np.isclose(calculate_tax(np.array(range(12))*100), 1215)

N = int(1E6)
arr = np.array([1]*N)
benchmark = timeit("calculate_tax(arr)", "from __main__ import calculate_tax, arr", number=1)
reference_benchmark = timeit("dummy(arr)", "from __main__ import dummy, arr", number=1)

assert reference_benchmark > benchmark*5, "Код работает слишком медленно — вы точно использовали методы numpy?"
def load_data(data_path): #LR1 ITIB MTUCI
    df = pd.read_csv(data_path)

    ##le = LabelEncoder()
    ##le.fit(df['housing_median_age'].astype(str))
    ##df['housing_median_age'] = le.transform(df['housing_median_age'].astype(str))
    y = df['housing_median_age']
    X = df.drop('housing_median_age', axis=1)
    X = df.drop('latitude', axis=1)
    i = len(X.columns)
    X = X.drop(X.columns[i - 1], axis=1)
    y.replace(('M', 'B'), (1, 0), inplace=True)
    sc = StandardScaler()
    sc.fit(X)
    X_ans = sc.transform(X)
    return X_ans, y

X, y = load_data("sample_data/california_housing_test.csv")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)

best_model = KNeighborsClassifier(
   n_neighbors=10, 
   weights='distance',
   algorithm='auto',
   leaf_size=30,
   metric='euclidean',
   metric_params=None,
   n_jobs=4
)
best_model.fit(X_train, y_train)
model_params = best_model.get_params()
tuned_params = {}
for k, v in model_params.items():
    tuned_params[k] = [v]
tuned_params['n_neighbors'] = range(1, 30)
clf = GridSearchCV(KNeighborsClassifier(), tuned_params, cv=10, n_jobs=-1)
clf.fit(X_train, y_train)
best_params = clf.best_params_
best_model = KNeighborsClassifier(**best_params)
best_model.fit(X_train, y_train)
predicted = best_model.predict(X_test)

class BrowserFPClassifier:
    def __init__(
        self,
        db_path='fingerprint.db',
        interaction_files=['clicks.csv','keystroke.csv','long_clicks.csv','paint.csv','scroll.csv','sensor_acc.csv','sensor_grav.csv','sensor_gyro.csv','sensor_lacc.csv','sensor_light.csv','sensor_magn.csv','swipe.csv','clicks.csv','user_info.csv','video.csv'],
        max_timesteps=10000,
        random_state=84,
        target_columns=['plugins', 'platform', 'timezone', 'hardwareConcurrency'],
        lstm_units=[1024, 128],            # NEW: list of LSTM layer sizes (first -> last)
        lstm_dropout=0.3                 # NEW: float or list of dropouts per LSTM layer
    ):
        self.db_path = db_path
        self.interaction_files = interaction_files
        self.max_timesteps = max_timesteps
        self.random_state = random_state
        self.target_columns = target_columns
        self.lstm_units = lstm_units
        self.lstm_dropout = lstm_dropout
        self.connection = None
        self.fingerprint_df = None
        self.interaction_df = None
        self.label_encoder = LabelEncoder()
        self.label_encoders = {}
        self.num_classes_dict = {}
        self.model = None  
    def connect_db(self):
        try:
            self.connection = sqlite3.connect(self.db_path)
        except sqlite3.Error as e:
            print(f"Error connecting to database: {e}")
            raise
    def load_fingerprint_data(self):
        try:
            self.fingerprint_df = pd.read_sql_query("SELECT * FROM fingerprints", self.connection)
            self.fingerprint_df['data'] = self.fingerprint_df['data'].apply(json.loads)
            self.fingerprint_df['plugins'] = self.fingerprint_df['data'].apply(
                lambda x: len(x.get('plugins', {}).get('value', []))
            )
            self.fingerprint_df['platform'] = self.fingerprint_df['data'].apply(
                lambda x: x.get('platform', {}).get('value', '')
            )
            self.fingerprint_df['timezone'] = self.fingerprint_df['data'].apply(
                lambda x: x.get('timezone', {}).get('value', '')
            )
            self.fingerprint_df['hardwareConcurrency'] = self.fingerprint_df['data'].apply(
                lambda x: x.get('hardwareConcurrency', {}).get('value', '')
            )
            # id column as int
            for i in self.fingerprint_df.index:
                self.fingerprint_df.loc[i, 'id'] = int(i)
            # Label encoding categorical features
            self.fingerprint_df['platform'] = self.label_encoder.fit_transform(self.fingerprint_df['platform'])
            self.fingerprint_df['timezone'] = self.label_encoder.fit_transform(self.fingerprint_df['timezone'])
            self.fingerprint_df = self.fingerprint_df[['id', 'plugins', 'platform', 'timezone', 'hardwareConcurrency']]
        except Exception as e:
            print(f"Error loading fingerprint data: {e}")
            raise
    def load_interaction_data(self):
        try:
            # Try to use the external fast C# service
            try:
                resp = requests.post(
                    "http://localhost:5000/process",
                    json={"files": self.interaction_files},
                    timeout=5
                )
                if resp.status_code == 200:
                    interaction_features = pd.DataFrame(resp.json())
                    # keep columns consistent with previous code
                    # expect columns: user_id_, session_, x_coordinate_mean, y_coordinate_mean
                    interaction_features.columns = [c if c in ['user_id_', 'session_', 'x_coordinate_mean', 'y_coordinate_mean'] else c for c in interaction_features.columns]
                else:
                    raise Exception(f"C# service returned status {resp.status_code}")
            except Exception as e:
                # Fallback: original Python CSV-based aggregation (kept simple)
                print(f"C# service unavailable or failed ({e}), falling back to Python processing.")
                df = pd.concat(map(pd.read_csv, self.interaction_files), ignore_index=True)
                df = df.drop_duplicates(subset=['user_id', 'session'], keep='first')
                df = df.drop_duplicates(subset=['user_id'], keep='first')
                interaction_features = df.groupby(['user_id', 'session']).agg({
                    'x_coordinate': ['mean'],
                    'y_coordinate': ['mean'],
                }).reset_index()
                interaction_features.columns = ['user_id_', 'session_', 'x_coordinate_mean', 'y_coordinate_mean']

            interaction_features['user_id_'] = interaction_features['user_id_'].astype(str)
            self.fingerprint_df['id'] = self.fingerprint_df['id'].astype(str)
            self.merged_df = pd.merge(self.fingerprint_df, interaction_features, left_on='id', right_on='user_id_', how='inner')
            self.merged_df = self.merged_df.drop(['user_id_', 'session_'], axis=1, errors='ignore')
            print(self.merged_df)
        except Exception as e:
            print(f"Error loading interaction data: {e}")
            raise
    def normalize_features(self):
        numeric_df = self.merged_df.select_dtypes(include=['float', 'int'])
        scaler = MinMaxScaler()
        self.normalized_df = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns)
        print(self.normalized_df)
    def encode_targets(self):
        y = self.merged_df[self.target_columns].copy()
        y_encoded = pd.DataFrame()
        for col in self.target_columns:
            le = LabelEncoder()
            y_encoded[col] = le.fit_transform(y[col].astype(str))
            self.label_encoders[col] = le
            self.num_classes_dict[col] = y_encoded[col].nunique()
        self.y_encoded = y_encoded
    def prepare_dynamic_data(self):
        sequences = []
        for _, row in self.merged_df.iterrows():
            seq = np.linspace(
                row['x_coordinate_mean'], 
                row['y_coordinate_mean'], 
                self.max_timesteps
            )
            sequences.append(seq)
        XY_dynamic = np.expand_dims(np.array(sequences), -1)
        return XY_dynamic
    def split_data(self):
        # X_static: Normalized features
        X_final = self.normalized_df
        XY_dynamic = self.prepare_dynamic_data()
        y_encoded = self.y_encoded
        X_train_static, X_test_static, X_train_dynamic, X_test_dynamic, y_train_df, y_test_df = train_test_split(
            X_final.values, XY_dynamic, y_encoded, random_state=self.random_state
        )
        # Outputs for each target
        y_train = [y_train_df[col].values for col in self.target_columns]
        y_test = [y_test_df[col].values for col in self.target_columns]
        self.X_train_static = X_train_static
        self.X_test_static = X_test_static
        self.X_train_dynamic = X_train_dynamic
        self.X_test_dynamic = X_test_dynamic
        self.y_train = y_train
        self.y_test = y_test
    def build_model(self):
        static_input = Input(shape=(self.X_train_static.shape[1],), name='static_input')
        dynamic_input = Input(shape=(self.max_timesteps, 1), name='dynamic_input')

        # Prepare dropout list
        if isinstance(self.lstm_dropout, (int, float)):
            dropouts = [self.lstm_dropout] * len(self.lstm_units)
        else:
            dropouts = list(self.lstm_dropout)
            # pad or trim to match units length
            if len(dropouts) < len(self.lstm_units):
                dropouts = (dropouts + [dropouts[-1]] * len(self.lstm_units))[:len(self.lstm_units)]

        # Stack LSTM layers: all but last return sequences=True
        x = dynamic_input
        for i, units in enumerate(self.lstm_units):
            is_last = (i == len(self.lstm_units) - 1)
            x = LSTM(units, return_sequences=not is_last)(x)
            x = Dropout(dropouts[i])(x)
        lstm_out = x

        dense_static = Dense(64, activation='selu')(static_input)
        dense_static = Dropout(0.3)(dense_static)
        combined = Concatenate()([lstm_out, dense_static])
        combined_dense = Dense(64, activation='selu')(combined)
        combined_dense = Dropout(0.5)(combined_dense)
        outputs = []
        for col in self.target_columns:
            outputs.append(Dense(self.num_classes_dict[col], activation='softmax', name=col)(combined_dense))
        model = Model(inputs=[dynamic_input, static_input], outputs=outputs)
        model.compile(
            loss='sparse_categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']*len(self.target_columns)
        )
        self.model = model
        self.model.summary()
    def plot_architecture(self, filename='model_architecture.png'):
        plot_model(self.model, to_file=filename, show_shapes=True, show_layer_names=True)
    def train(self, epochs=100, batch_size=32):
        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]
        history = self.model.fit(
            {'dynamic_input': self.X_train_dynamic, 'static_input': self.X_train_static},
            {col: self.y_train[i] for i, col in enumerate(self.target_columns)},
            validation_data=(
                {'dynamic_input': self.X_test_dynamic, 'static_input': self.X_test_static},
                {col: self.y_test[i] for i, col in enumerate(self.target_columns)}
            ),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        return history
    def evaluate_and_visualize(self):
        y_pred = self.model.predict({'dynamic_input': self.X_test_dynamic, 'static_input': self.X_test_static})
        results = self.model.evaluate(
            {'dynamic_input': self.X_test_dynamic, 'static_input': self.X_test_static},
            {col: self.y_test[i] for i, col in enumerate(self.target_columns)}
        )
        for i, col in enumerate(self.target_columns):
            print(f"\nМетрики для выхода '{col}':")
            y_true_i = self.y_test[i]
            y_pred_i = y_pred[i]
            y_pred_classes = np.argmax(y_pred_i, axis=1)
            unique_classes_true = np.unique(y_true_i)
            unique_classes_pred = np.unique(y_pred_classes)
            print(f"Уникальные истинные классы для {col}: {unique_classes_true}")
            try:
                acc = accuracy_score(y_true_i, y_pred_classes)
                rec_macro = recall_score(y_true_i, y_pred_classes, average='macro')
                f1_macro = f1_score(y_true_i, y_pred_classes, average='macro')
                print(f"Accuracy: {acc:.4f}")
                print(f"Recall (macro): {rec_macro:.4f}")
                print(f"F1-score (macro): {f1_macro:.4f}")
            except Exception as e:
                print(f"Ошибка (Accuracy/Recall/F1): {str(e)}")
            # Probabilistic metrics (ROC-AUC, Average precision)
            try:
                n_classes_predicted = y_pred_i.shape[1]
                y_true_one_hot = tf.keras.utils.to_categorical(y_true_i, num_classes=n_classes_predicted)
                for class_idx in range(n_classes_predicted):
                    fpr, tpr, _ = roc_curve(y_true_one_hot[:, class_idx], y_pred_i[:, class_idx])
                    plt.plot(fpr, tpr, lw=2, label=f'Класс {class_idx} (AUC = {auc(fpr, tpr):.2f})')
                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
                plt.xlabel('False Positive Rate (FPR)')
                plt.ylabel('True Positive Rate (TPR)')
                plt.title(f'AUC-ROC кривые для всех классов: {col}')
                plt.legend(loc='lower right')
                plt.show()
                avg_precision = average_precision_score(
                    y_true_one_hot,
                    y_pred_i,
                    average='macro'
                )
                print(f"Avg Precision (macro): {avg_precision:.4f}")
                roc_auc = roc_auc_score(
                    y_true_one_hot,
                    y_pred_i,
                    multi_class='ovo',
                    average='macro'
                )
                print(f"ROC AUC (macro): {roc_auc:.4f}")
            except Exception as e:
                print(f"Ошибка (Avg Precision / ROC AUC): {str(e)}")
pipeline = BrowserFPClassifier()
pipeline.connect_db()
pipeline.load_fingerprint_data()
pipeline.load_interaction_data()
pipeline.normalize_features()
pipeline.encode_targets()
pipeline.split_data()
pipeline.build_model()
pipeline.plot_architecture()
pipeline.train(epochs=100, batch_size=32)
pipeline.evaluate_and_visualize()